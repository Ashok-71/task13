# ğŸ“ FOLDER STRUCTURE GUIDE

## Complete Project Structure

```
web-scraping-task/
â”‚
â”œâ”€â”€ web_scraper.py          # Main Python scraping script
â”œâ”€â”€ scraped_data.csv        # CSV file with extracted data (sample included)
â”œâ”€â”€ requirements.txt        # Python package dependencies
â”œâ”€â”€ README.md              # Complete project documentation
â”œâ”€â”€ .gitignore             # Git ignore file
â”‚
â””â”€â”€ screenshots/           # Optional folder for screenshots
    â””â”€â”€ output_preview.png # (Add your own screenshot)
```

## ğŸ“ How to Set Up This Structure

### Method 1: Manual Setup

1. Create a new folder: `web-scraping-task`
2. Copy all provided files into this folder
3. (Optional) Create `screenshots/` folder for output images

### Method 2: Using Terminal

```bash
# Create main directory
mkdir web-scraping-task
cd web-scraping-task

# Create optional screenshots folder
mkdir screenshots

# Copy/paste the provided files:
# - web_scraper.py
# - requirements.txt
# - README.md
# - .gitignore
# - scraped_data.csv (this will be regenerated when you run the script)
```

## ğŸš€ GitHub Setup

### Step 1: Initialize Git Repository
```bash
cd web-scraping-task
git init
```

### Step 2: Add Files
```bash
git add .
git commit -m "Initial commit: Task 13 - Web Scraping"
```

### Step 3: Push to GitHub
```bash
# Create a new repository on GitHub first, then:
git remote add origin https://github.com/yourusername/web-scraping-task.git
git branch -M main
git push -u origin main
```

## âœ… Files Checklist

Before submission, ensure you have:

- âœ… `web_scraper.py` - Your scraping script
- âœ… `scraped_data.csv` - Generated CSV with data
- âœ… `requirements.txt` - Dependencies list
- âœ… `README.md` - Project documentation
- âœ… `.gitignore` - Git ignore rules
- âœ… (Optional) Screenshots of output

## ğŸ“¤ Submission

1. Push everything to GitHub
2. Copy your repository link (e.g., `https://github.com/yourusername/web-scraping-task`)
3. Submit the link before 10:00 PM

---

**Note:** The `scraped_data.csv` file included is a sample. When you run `web_scraper.py`, it will generate a fresh CSV with actual scraped data.
